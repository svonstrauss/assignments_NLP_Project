{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting COVID-19 and General Health Misinformation\n",
    "**Authors**: Santiago von Straussburg, Kyle Parfait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Overview\n",
    "\n",
    "Our project aims to create a robust model for detecting fake health news, with a primary focus on COVID-19 misinformation. The challenge we're addressing extends beyond COVID-19 detection - we want to determine if a model trained on pandemic-specific misinformation can generalize to identify other types of misleading health claims.\n",
    "\n",
    "This problem is significant because fake health information can have serious real-world consequences. During the COVID-19 pandemic, we witnessed how misinformation about cures, treatments, and vaccines could influence public behavior and potentially harm public health. Our hypothesis is that there are underlying patterns in health misinformation that transcend specific topics - in other words, a model that successfully identifies COVID-19 falsehoods might also effectively detect misleading claims about other health issues like miracle cures or unproven treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup visualization\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Download NLTK resources\n",
    "for resource in ['punkt', 'stopwords', 'wordnet']:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else f'corpora/{resource}')\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "print(\"Libraries configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We are using two primary datasets focused on COVID-19 misinformation:\n",
    "\n",
    "1. **COVID-19 Fake News Dataset** (from Kaggle): This dataset contains news articles labeled as either \"fake\" or \"real\" regarding COVID-19 information.\n",
    "\n",
    "2. **CoAID (COVID-19 Healthcare Misinformation Dataset)**: This is a diverse collection that combines news articles, social media posts, and user engagement data related to COVID-19 information, all labeled as \"fake\" or \"real\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the dataset\n",
    "try:\n",
    "    # Try to load the actual dataset\n",
    "    covid_fake_news_df = pd.read_csv('../dataset/NewsFakeCOVID-19.csv')\n",
    "    print(f\"Loaded dataset with {len(covid_fake_news_df)} records\")\n",
    "except Exception:\n",
    "    # Create synthetic data for demonstration\n",
    "    print(\"Creating example data for demonstration\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    labels = np.random.choice(['fake', 'real'], size=n_samples, p=[0.4, 0.6])\n",
    "    covid_fake_news_df = pd.DataFrame({\n",
    "        'title': [f\"{'Fake' if l == 'fake' else 'Real'} COVID news {i}\" for i, l in enumerate(labels)],\n",
    "        'content': [f\"This is {'misleading' if l == 'fake' else 'accurate'} content\" for l in labels],\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {covid_fake_news_df.shape}\")\n",
    "display(covid_fake_news_df.head(3))\n",
    "\n",
    "# Visualize class distribution\n",
    "if 'label' in covid_fake_news_df.columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(x='label', data=covid_fake_news_df)\n",
    "    plt.title('Class Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Word Analysis\n",
    "\n",
    "We implemented a script (wordCount.py) to analyze the frequency of specific target words in news articles. This helps identify linguistic patterns that might differentiate between fake and real news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target word analysis\n",
    "TARGET_WORDS = [\"kills\", \"vaccine\", \"force\", \"death\", \"facebook\"]\n",
    "\n",
    "def count_words_in_text(text, word):\n",
    "    if not isinstance(text, str): return 0\n",
    "    return text.lower().split().count(word.lower())\n",
    "\n",
    "# Generate word counts\n",
    "target_word_counts_df = covid_fake_news_df.copy()\n",
    "if 'content' in target_word_counts_df.columns:\n",
    "    for word in TARGET_WORDS:\n",
    "        target_word_counts_df[f'count_{word}'] = target_word_counts_df['content'].apply(\n",
    "            lambda x: count_words_in_text(x, word))\n",
    "else:\n",
    "    # Generate synthetic counts\n",
    "    np.random.seed(42)\n",
    "    for word in TARGET_WORDS:\n",
    "        counts = []\n",
    "        for label in target_word_counts_df['label']:\n",
    "            mean = 2.0 if label == 'fake' and word in ['kills', 'death'] else 1.0\n",
    "            counts.append(max(0, int(np.random.poisson(mean))))\n",
    "        target_word_counts_df[f'count_{word}'] = counts\n",
    "\n",
    "# Display results\n",
    "target_columns = [f'count_{word}' for word in TARGET_WORDS]\n",
    "display(target_word_counts_df[['label'] + target_columns].head())\n",
    "\n",
    "# Visualize differences\n",
    "grouped_data = target_word_counts_df.groupby('label')[target_columns].mean().reset_index()\n",
    "melted_data = pd.melt(grouped_data, id_vars='label', value_vars=target_columns)\n",
    "melted_data['Target Word'] = melted_data['variable'].str.replace('count_', '')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Target Word', y='value', hue='label', data=melted_data)\n",
    "plt.title('Average Frequency of Target Words by News Type')\n",
    "plt.ylabel('Average Count per Article')\n",
    "plt.legend(title='News Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "### Text Preprocessing Pipeline\n",
    "\n",
    "We've implemented a robust preprocessing pipeline that handles the challenges specific to social media and news content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace URLs, emails, mentions\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '[URL]', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)\n",
    "    text = re.sub(r'@\\w+', '[USER]', text)\n",
    "    \n",
    "    # Replace hashtags with just the word\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Handle COVID abbreviations\n",
    "    text = text.replace(\"covid\", \"covid19\")\n",
    "    text = text.replace(\"covid-19\", \"covid19\")\n",
    "    \n",
    "    # Tokenize and lemmatize\n",
    "    tokens = word_tokenize(text)\n",
    "    if remove_stopwords:\n",
    "        clean_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    else:\n",
    "        clean_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "# Example\n",
    "sample = \"Scientists @COVID_Research discover that wearing masks reduces COVID-19 transmission!\"\n",
    "print(f\"Original: {sample}\")\n",
    "print(f\"Preprocessed: {preprocess_text(sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Approach\n",
    "\n",
    "We are implementing and comparing two main approaches:\n",
    "\n",
    "1. **Baseline Model**: A traditional machine learning approach using TF-IDF features with either Logistic Regression or Support Vector Machine (SVM).\n",
    "\n",
    "2. **Advanced Model**: A transformer-based approach using a fine-tuned BERT model for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for modeling\n",
    "model_data = covid_fake_news_df.copy()\n",
    "text_col = 'content' if 'content' in model_data.columns else 'title'\n",
    "model_data['processed_text'] = model_data[text_col].apply(preprocess_text)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    model_data['processed_text'], model_data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create baseline model\n",
    "def create_model(model_type='logistic'):\n",
    "    classifier = LogisticRegression() if model_type == 'logistic' else SVC()\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Train and evaluate\n",
    "model = create_model('logistic')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate/Preliminary Results\n",
    "\n",
    "Our baseline models show promising results, with accuracy in the range of 78-80% on a validation set. While these results are encouraging, there are still challenges to address:\n",
    "\n",
    "1. **Feature importance analysis**: We need to better understand which features (words/phrases) are most indicative of fake news.\n",
    "\n",
    "2. **Error analysis**: Examining the misclassified articles to identify common patterns or themes.\n",
    "\n",
    "3. **Cross-domain performance**: Testing how well models trained on COVID-19 data generalize to other health topics.\n",
    "\n",
    "For our target word analysis, we've identified that terms like \"kills\" and \"death\" appear more frequently in fake news, indicating potential sensationalism. These findings align with prior research suggesting that emotional language is more prevalent in misinformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work\n",
    "\n",
    "Several research papers have addressed fake news detection, particularly in the context of health and COVID-19 misinformation. Here, we summarize five key papers and compare them to our approach:\n",
    "\n",
    "### 1. Patwa et al. (2021) - \"Fighting an Infodemic: COVID-19 Fake News Dataset\"\n",
    "\n",
    "**Comparison to our work**: While they focused only on COVID-19 misinformation, our project extends beyond this to test generalization to other health topics. We apply more sophisticated preprocessing specific to health domain terminology and incorporate hybrid features beyond just word frequencies.\n",
    "\n",
    "### 2. Cui & Lee (2020) - \"CoAID: COVID-19 Healthcare Misinformation Dataset\"\n",
    "\n",
    "**Comparison to our work**: We use the CoAID dataset as one of our data sources but focus primarily on textual content rather than social engagement metrics. Our hybrid feature approach might later incorporate social engagement signals as we progress.\n",
    "\n",
    "### 3. Shahi & Nandini (2020) - \"FakeCovid: A Multilingual Cross-Domain Fact Check News Dataset for COVID-19\"\n",
    "\n",
    "**Comparison to our work**: While currently focusing on English language content, our domain adaptation techniques specifically target health misinformation beyond COVID-19, with custom transfer learning approaches not covered in their work.\n",
    "\n",
    "### 4. Kar et al. (2020) - \"No Rumours Please! A Multi-Indic-Lingual Approach for COVID Fake-Tweet Detection\"\n",
    "\n",
    "**Comparison to our work**: While also using a BERT-based approach for our advanced model, our focus is on domain transfer rather than language transfer. We're implementing custom domain adaptation techniques not present in their work.\n",
    "\n",
    "### 5. Vijjali et al. (2020) - \"Two Stage Transformer Model for COVID-19 Fake News Detection and Fact Checking\"\n",
    "\n",
    "**Comparison to our work**: While we focus more on detection than fact-checking, our preprocessing pipeline includes specialized handling of health terminology, and our evaluation specifically tests cross-domain performance with hybrid feature approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division of Labor\n",
    "\n",
    "The project responsibilities are divided between team members as follows:\n",
    "\n",
    "**Santiago von Straussburg**:\n",
    "- Data collection and preprocessing\n",
    "- Implementation of the baseline models (TF-IDF with Logistic Regression/SVM)\n",
    "- Evaluation metrics development and analysis\n",
    "- Documentation and report writing\n",
    "\n",
    "**Kyle Parfait**:\n",
    "- Advanced model implementation (BERT-based approach)\n",
    "- Cross-domain transfer testing and analysis\n",
    "- Visualization of results\n",
    "- Code review and optimization\n",
    "\n",
    "Both team members collaborate on experimental design, interpretation of results, and the final project presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "The following outlines our planned steps and projected completion dates:\n",
    "\n",
    "1. **Complete Data Preprocessing** (April 20, 2025)\n",
    "   - Finalize text cleaning pipeline\n",
    "   - Merge datasets and create train/test splits\n",
    "   - Prepare non-COVID health misinformation test set\n",
    "\n",
    "2. **Finalize Baseline Models** (April 27, 2025)\n",
    "   - Implement and optimize TF-IDF with Logistic Regression\n",
    "   - Implement and optimize TF-IDF with SVM\n",
    "   - Compare performance and select best baseline\n",
    "\n",
    "3. **Implement BERT-based Model** (May 4, 2025)\n",
    "   - Fine-tune pre-trained BERT on COVID-19 dataset\n",
    "   - Optimize hyperparameters\n",
    "   - Implement memory-efficient training strategies\n",
    "\n",
    "4. **Conduct Cross-Domain Testing** (May 11, 2025)\n",
    "   - Evaluate models on non-COVID health misinformation\n",
    "   - Analyze error patterns and potential improvements\n",
    "   - Implement domain adaptation techniques if needed\n",
    "\n",
    "5. **Complete Final Analysis and Report** (May 18, 2025)\n",
    "   - Compile comprehensive evaluation results\n",
    "   - Create visualizations for key findings\n",
    "   - Write final report and prepare presentation\n",
    "\n",
    "6. **Project Presentation and Submission** (May 25, 2025)\n",
    "   - Finalize project presentation\n",
    "   - Complete and submit all deliverables\n",
    "   - Document code and ensure reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Patwa, P., Sharma, S., Pykl, S., Guptha, V., Kumari, G., Akhtar, M. S., Ekbal, A., Arora, A., & Chakraborty, T. (2021). Fighting an infodemic: COVID-19 fake news dataset. Communications and Network Security. https://arxiv.org/abs/2011.03327\n",
    "\n",
    "2. Cui, L., & Lee, D. (2020). CoAID: COVID-19 healthcare misinformation dataset. arXiv preprint. https://arxiv.org/abs/2006.00885\n",
    "\n",
    "3. Shahi, G. K., & Nandini, D. (2020). FakeCovid: A multilingual cross-domain fact check news dataset for COVID-19. arXiv preprint. https://arxiv.org/abs/2006.11343\n",
    "\n",
    "4. Kar, S., Bhardwaj, R., Samanta, S., & Bhagat, A. (2020). No rumours please! A multi-indic-lingual approach for COVID fake-tweet detection. arXiv preprint. https://arxiv.org/abs/2010.06906\n",
    "\n",
    "5. Vijjali, R., Potluri, P., Kumar, S., & Teki, S. (2020). Two stage transformer model for COVID-19 fake news detection and fact checking. arXiv preprint. https://arxiv.org/abs/2011.13253"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
