{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting COVID-19 and General Health Misinformation\n",
    "**Authors**: Santiago von Straussburg, Kyle Parfait\n",
    "\n",
    "## Milestone Report\n",
    "This notebook contains our milestone report for the project on detecting COVID-19 and general health misinformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Overview\n",
    "\n",
    "Our project aims to create a robust model for detecting fake health news, with a primary focus on COVID-19 misinformation. The challenge we're addressing extends beyond COVID-19 detection - we want to determine if a model trained on pandemic-specific misinformation can generalize to identify other types of misleading health claims.\n",
    "\n",
    "This problem is significant because fake health information can have serious real-world consequences. During the COVID-19 pandemic, we witnessed how misinformation about cures, treatments, and vaccines could influence public behavior and potentially harm public health. Our hypothesis is that there are underlying patterns in health misinformation that transcend specific topics - in other words, a model that successfully identifies COVID-19 falsehoods might also effectively detect misleading claims about other health issues like miracle cures or unproven treatments.\n",
    "\n",
    "If our approach successfully transfers from COVID-19 to other health misinformation, it would demonstrate that our system is not merely memorizing pandemic-specific language patterns but is actually learning meaningful characteristics of health misinformation in general. This would be a significant contribution to the ongoing battle against health-related fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We are using two primary datasets focused on COVID-19 misinformation:\n",
    "\n",
    "1. **COVID-19 Fake News Dataset** (from Kaggle): This dataset contains news articles labeled as either \"fake\" or \"real\" regarding COVID-19 information.\n",
    "\n",
    "2. **CoAID (COVID-19 Healthcare Misinformation Dataset)**: This is a diverse collection that combines news articles, social media posts, and user engagement data related to COVID-19 information, all labeled as \"fake\" or \"real\".\n",
    "\n",
    "### Data Collection Process\n",
    "\n",
    "We obtained these datasets from their respective sources:\n",
    "\n",
    "1. The COVID-19 Fake News Dataset was downloaded from Kaggle, where it was compiled by researchers collecting news articles and fact-checking their veracity during the pandemic.\n",
    "\n",
    "2. The CoAID dataset was accessed through GitHub, where it was published by researchers at Pennsylvania State University. This dataset was compiled by collecting both news articles and social media content, which was then labeled through a combination of fact-checking website references and expert review.\n",
    "\n",
    "### Data Challenges\n",
    "\n",
    "The collection and preparation of these datasets presented several challenges:\n",
    "\n",
    "1. **Noisy data from social media**: Social media content often contains slang, abbreviations, and non-standard language which makes preprocessing more complex.\n",
    "\n",
    "2. **Class imbalance**: There are typically fewer examples of fake news compared to legitimate news, which can bias model training.\n",
    "\n",
    "3. **Topic specificity**: COVID-19 data contains pandemic-specific terminology that may not generalize to other health topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Example code for loading datasets\n",
    "# covid_fake_news_df = pd.read_csv('../dataset/NewsFakeCOVID-19.csv')\n",
    "# fake_real_news_df = pd.read_csv('../dataset/fake_and_real_news.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "Our approach to detecting fake health news involves a multi-step process:\n",
    "\n",
    "### Text Preprocessing Pipeline\n",
    "\n",
    "1. **Tokenization**: Breaking text into individual tokens (words, punctuation)\n",
    "2. **Lowercasing**: Converting all text to lowercase to reduce dimensionality\n",
    "3. **Stop word removal**: Removing common words that don't contribute much meaning\n",
    "4. **Lemmatization**: Reducing words to their base forms\n",
    "5. **Special handling for URLs and mentions**: Replacing or removing these elements\n",
    "\n",
    "### Modeling Approach\n",
    "\n",
    "We are implementing and comparing two main approaches:\n",
    "\n",
    "1. **Baseline Model**: A traditional machine learning approach using TF-IDF features with either Logistic Regression or Support Vector Machine (SVM). This serves as a benchmark for comparison.\n",
    "\n",
    "2. **Advanced Model**: A transformer-based approach using a fine-tuned BERT model, which has shown strong performance in various text classification tasks.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "We will evaluate our models using the following metrics:\n",
    "\n",
    "1. **Accuracy**: The proportion of correctly classified instances\n",
    "2. **Precision**: The proportion of true positive predictions among all positive predictions\n",
    "3. **Recall**: The proportion of true positive predictions among all actual positives\n",
    "4. **F1-Score**: The harmonic mean of precision and recall\n",
    "5. **Confusion Matrix**: A visualization of prediction errors and correct classifications\n",
    "\n",
    "We'll also perform cross-domain evaluation by testing our COVID-19-trained model on non-COVID health misinformation to assess generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate/Preliminary Experiments & Results\n",
    "\n",
    "At this milestone, we have conducted several preliminary experiments and analyses:\n",
    "\n",
    "### Target Word Analysis\n",
    "\n",
    "We analyzed the frequency of specific target words (\"kills\", \"vaccine\", \"force\", \"death\", \"facebook\") in news articles. This analysis helps us understand linguistic patterns that might differentiate fake from real news. The `wordCount.py` script was used to process a sample of articles and count these target words.\n",
    "\n",
    "### Data Preparation Progress\n",
    "\n",
    "We have completed the following steps in data preparation:\n",
    "\n",
    "1. **Data Collection**: Acquired the COVID-19 Fake News Dataset and the CoAID collection\n",
    "2. **Initial Data Exploration**: Analyzed dataset structure, class distribution, and basic statistics\n",
    "3. **Text Preprocessing Pipeline**: Developed and tested preprocessing functions for cleaning text data\n",
    "4. **Target Word Analysis**: Analyzed frequency of specific words that might indicate fake news\n",
    "\n",
    "### Preliminary Model Testing\n",
    "\n",
    "We have designed the framework for our baseline models (TF-IDF with Logistic Regression or SVM) and are in the process of implementing our advanced BERT-based approach. Initial tests on small subsets of data show promising results, though comprehensive evaluation is still pending.\n",
    "\n",
    "### Challenges and Adjustments\n",
    "\n",
    "During our preliminary work, we've encountered several challenges:\n",
    "\n",
    "1. **Data Quality Issues**: Some news URLs were inaccessible or returned empty content. We've implemented robust error handling to deal with these cases.\n",
    "\n",
    "2. **Computational Constraints**: BERT models are computationally intensive. We're exploring methods to optimize memory usage, such as gradient accumulation and mixed-precision training.\n",
    "\n",
    "3. **Domain Transfer Challenge**: Initial tests suggest that models trained solely on COVID-19 data struggle with non-COVID health misinformation. We're investigating domain adaptation techniques to improve cross-domain performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work\n",
    "\n",
    "Several research papers have addressed fake news detection, particularly in the context of health and COVID-19 misinformation. Here, we summarize five key papers and compare them to our approach:\n",
    "\n",
    "### 1. Patwa et al. (2021) - \"Fighting an Infodemic: COVID-19 Fake News Dataset\"\n",
    "\n",
    "This paper introduced a large-scale COVID-19 fake news dataset and tested various classical machine learning and deep learning models. Interestingly, they found that simpler models like SVM and logistic regression sometimes outperformed more complex architectures.\n",
    "\n",
    "**Comparison to our work**: While Patwa et al. focused purely on COVID-19 misinformation, our project extends beyond this to test generalization to other health topics. We are also using their finding about simpler models sometimes outperforming complex ones to justify our baseline comparison approach.\n",
    "\n",
    "### 2. Cui & Lee (2020) - \"CoAID: COVID-19 Healthcare Misinformation Dataset\"\n",
    "\n",
    "This paper introduced the CoAID dataset, which combines news articles, social media posts, and user engagement metrics. A unique aspect of their work is the inclusion of social interaction data (likes, shares) and how these correlate with the spread of misinformation.\n",
    "\n",
    "**Comparison to our work**: We're using the CoAID dataset but focusing primarily on the textual content rather than social engagement metrics. However, their insights about the viral potential of health misinformation inform our understanding of why this problem is important.\n",
    "\n",
    "### 3. Shahi & Nandini (2020) - \"FakeCovid: A Multilingual Cross-Domain Fact Check News Dataset for COVID-19\"\n",
    "\n",
    "This paper compiled a multilingual dataset of fact-checked COVID-19 articles from numerous countries. Their focus was on cross-lingual and cross-domain analysis, examining how fake news varies across different cultural contexts.\n",
    "\n",
    "**Comparison to our work**: While we're currently focusing on English language content, Shahi & Nandini's cross-domain approach aligns with our goal of testing generalization from COVID-19 to other health topics. Their findings about challenges in cross-domain detection inform our expectations about transfer learning difficulties.\n",
    "\n",
    "### 4. Kar et al. (2020) - \"No Rumours Please! A Multi-Indic-Lingual Approach for COVID Fake-Tweet Detection\"\n",
    "\n",
    "This study tackled multilingual fake tweet detection using a BERT-based framework. Their work demonstrated that even with limited labeled data, pre-trained models can achieve good results when fine-tuned appropriately.\n",
    "\n",
    "**Comparison to our work**: Like Kar et al., we're using a BERT-based approach, though our focus is on domain transfer rather than language transfer. Their finding that domain-specific features boost detection accuracy is relevant to our project, suggesting we might benefit from incorporating health-specific features.\n",
    "\n",
    "### 5. Vijjali et al. (2020) - \"Two Stage Transformer Model for COVID-19 Fake News Detection and Fact Checking\"\n",
    "\n",
    "This paper proposed an innovative two-stage approach: first retrieving relevant facts from a knowledge base, then using textual entailment to verify claims. This combines detection with automated fact-checking.\n",
    "\n",
    "**Comparison to our work**: Our current approach is more focused on the detection stage, without the fact-checking component that Vijjali et al. implemented. However, their transformer-based architecture is similar to our BERT approach, and their results provide a benchmark for what's achievable with transformer models on COVID-19 misinformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division of Labor\n",
    "\n",
    "The project responsibilities are divided between team members as follows:\n",
    "\n",
    "**Santiago von Straussburg**:\n",
    "- Data collection and preprocessing\n",
    "- Implementation of the baseline models (TF-IDF with Logistic Regression/SVM)\n",
    "- Evaluation metrics development and analysis\n",
    "- Documentation and report writing\n",
    "\n",
    "**Kyle Parfait**:\n",
    "- Advanced model implementation (BERT-based approach)\n",
    "- Cross-domain transfer testing and analysis\n",
    "- Visualization of results\n",
    "- Code review and optimization\n",
    "\n",
    "Both team members collaborate on experimental design, interpretation of results, and the final project presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "The following outlines our planned steps and projected completion dates:\n",
    "\n",
    "1. **Complete Data Preprocessing** (April 20, 2025)\n",
    "   - Finalize text cleaning pipeline\n",
    "   - Merge datasets and create train/test splits\n",
    "   - Prepare non-COVID health misinformation test set\n",
    "\n",
    "2. **Finalize Baseline Models** (April 27, 2025)\n",
    "   - Implement and optimize TF-IDF with Logistic Regression\n",
    "   - Implement and optimize TF-IDF with SVM\n",
    "   - Compare performance and select best baseline\n",
    "\n",
    "3. **Implement BERT-based Model** (May 4, 2025)\n",
    "   - Fine-tune pre-trained BERT on COVID-19 dataset\n",
    "   - Optimize hyperparameters\n",
    "   - Implement memory-efficient training strategies\n",
    "\n",
    "4. **Conduct Cross-Domain Testing** (May 11, 2025)\n",
    "   - Evaluate models on non-COVID health misinformation\n",
    "   - Analyze error patterns and potential improvements\n",
    "   - Implement domain adaptation techniques if needed\n",
    "\n",
    "5. **Complete Final Analysis and Report** (May 18, 2025)\n",
    "   - Compile comprehensive evaluation results\n",
    "   - Create visualizations for key findings\n",
    "   - Write final report and prepare presentation\n",
    "\n",
    "6. **Project Presentation and Submission** (May 25, 2025)\n",
    "   - Finalize project presentation\n",
    "   - Complete and submit all deliverables\n",
    "   - Document code and ensure reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Patwa, P., Sharma, S., Pykl, S., Guptha, V., Kumari, G., Akhtar, M. S., Ekbal, A., Arora, A., & Chakraborty, T. (2021). Fighting an infodemic: COVID-19 fake news dataset. Communications and Network Security. https://arxiv.org/abs/2011.03327\n",
    "\n",
    "2. Cui, L., & Lee, D. (2020). CoAID: COVID-19 healthcare misinformation dataset. arXiv preprint. https://arxiv.org/abs/2006.00885\n",
    "\n",
    "3. Shahi, G. K., & Nandini, D. (2020). FakeCovid: A multilingual cross-domain fact check news dataset for COVID-19. arXiv preprint. https://arxiv.org/abs/2006.11343\n",
    "\n",
    "4. Kar, S., Bhardwaj, R., Samanta, S., & Bhagat, A. (2020). No rumours please! A multi-indic-lingual approach for COVID fake-tweet detection. arXiv preprint. https://arxiv.org/abs/2010.06906\n",
    "\n",
    "5. Vijjali, R., Potluri, P., Kumar, S., & Teki, S. (2020). Two stage transformer model for COVID-19 fake news detection and fact checking. arXiv preprint. https://arxiv.org/abs/2011.13253"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
