{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this assignment, fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`, as well as your name below.\n",
    "\n",
    "To make sure everything runs as expected, do the following\n",
    "- **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart)\n",
    "- **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "A good introduction to Jupyter notebooks is [here](https://realpython.com/jupyter-notebook-introduction/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>HW2 (50 points)</h1> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Part-I:-Word-Vectors\" data-toc-modified-id=\"Part-I:-Word-Vectors-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Part I: Word Vectors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-embeddings-for-classification\" data-toc-modified-id=\"Using-embeddings-for-classification-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Using embeddings for classification</a></span></li><li><span><a href=\"#Intepreting-word-vector-models\" data-toc-modified-id=\"Intepreting-word-vector-models-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Intepreting word vector models</a></span></li></ul></li><li><span><a href=\"#Part-II:-Sequence-Models\" data-toc-modified-id=\"Part-II:-Sequence-Models-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Part II: Sequence Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#transition-probabilities\" data-toc-modified-id=\"transition-probabilities-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>transition probabilities</a></span></li><li><span><a href=\"#emission-probabilities\" data-toc-modified-id=\"emission-probabilities-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>emission probabilities</a></span></li><li><span><a href=\"#start-probabilities\" data-toc-modified-id=\"start-probabilities-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>start probabilities</a></span></li><li><span><a href=\"#Viterbi\" data-toc-modified-id=\"Viterbi-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Viterbi</a></span></li><li><span><a href=\"#running-your-HMM-on-real-data\" data-toc-modified-id=\"running-your-HMM-on-real-data-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>running your HMM on real data</a></span></li></ul></li><li><span><a href=\"#Part-III:-RNNs-for-POS-tagging\" data-toc-modified-id=\"Part-III:-RNNs-for-POS-tagging-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Part III: RNNs for POS tagging</a></span><ul class=\"toc-item\"><li><span><a href=\"#RNNs-on-real-data.\" data-toc-modified-id=\"RNNs-on-real-data.-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>RNNs on real data.</a></span></li><li><span><a href=\"#RNNs-with-word-embeddings\" data-toc-modified-id=\"RNNs-with-word-embeddings-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>RNNs with word embeddings</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec7d43d45eb8f9384587176878f147dd",
     "grade": false,
     "grade_id": "jupyter",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "\n",
    "In this assignment we will:\n",
    "- explore word embeddings for classification\n",
    "- implement HMMs for part-of-speech tagging\n",
    "- apply RNNs for part-of-speech tagging\n",
    "\n",
    "As in the previous assignment, there are spaces below for you to both write code and short answers. In some places, there are tests to check your work, though passing tests does not guarantee full credit. I recommend moving sequentially from top to bottom, getting each step working before moving on to the next.\n",
    "\n",
    "This assignment will use a number of Python libraries, listed in the next cell. If you haven't already installed these, you can do so by running this command in this directory: `pip install -r requirements.txt`. Minor variants in the version numbers shouldn't affect things much.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter, defaultdict\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from numpy import array as npa\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5, rc={'figure.figsize':(12, 6)})\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Word Vectors\n",
    "\n",
    "In this section, we'll load some pre-trained word embeddings and use them to re-classify the movie review data from HW1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small word embedding model.\n",
    "# Here we're using a variant called GloVe, which is trained a bit differently than we've done in class.\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "# The details will not matter much for the purposes of this assignment.\n",
    "w2v = api.load('glove-wiki-gigaword-50')\n",
    "# see list of possible downloads here: https://github.com/RaRe-Technologies/gensim-data\n",
    "print('word embeddings for %d words of dimension %d' % (len(w2v.key_to_index), len(w2v['love'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's project these 50 dimensions down to 2 to visualize some important terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings_w2v(w2v, words):\n",
    "    word_vectors = np.array([w2v[a] for a in words])\n",
    "    # reduce two 2 dimensions\n",
    "    twodim = PCA(random_state=42).fit_transform(word_vectors)[:,:2]\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+.05, y+.05, word)\n",
    "    plt.show()\n",
    "    \n",
    "plot_embeddings_w2v(w2v, ['great', 'best', 'worth', 'again', 'watch', 'excellent', 'characters', 'where', 'role',\n",
    "                          'worst', 'nothing', 'bad', 'terrible', 'waste', 'boring', 'why', 'awful', 'because', 'poor'])\n",
    "# seems to be some signal separating words by sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using embeddings for classification\n",
    "\n",
    "Our first task is to use these embeddings for classification. As mentioned in class, we have a few options for ways of aggregating the vectors for all the words in a document. For example:\n",
    "\n",
    "1. `average`: average the vectors\n",
    "2. `max`: take the max along each dimension\n",
    "3. `min`: take the min along each dimension\n",
    "4. `min` $\\oplus$ `max`: concatenate `min` and `max` vectors (total length = 50*2)\n",
    "\n",
    "We'll reclassify the movie review data from the previous assignment using word vectors to see if it makes a difference in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c5ce3399935f48053aabebc84b1534f",
     "grade": false,
     "grade_id": "avg",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def average_vectors(words, w2v):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "average_vectors(['great', 'wonderful'], w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa1985c666057ec0488140e60f7bc90f",
     "grade": true,
     "grade_id": "avg2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(average_vectors(['great', 'wonderful'], w2v)[0].item(), 2) == .10\n",
    "assert round(average_vectors(['great', 'wonderful'], w2v)[1].item(), 2) == 1.12\n",
    "assert average_vectors(['great', 'wonderful'], w2v).shape[0] == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb271d655b0133bc021426d2b90452b4",
     "grade": false,
     "grade_id": "max",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def max_vectors(words, w2v):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "max_vectors(['great', 'wonderful'], w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a2cbc81be791c9ff87304810085d06b",
     "grade": true,
     "grade_id": "max2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(max_vectors(['great', 'wonderful'], w2v)[0].item(), 2) == .24\n",
    "assert round(max_vectors(['great', 'wonderful'], w2v)[1].item(), 2) == 1.34\n",
    "assert max_vectors(['great', 'wonderful'], w2v).shape[0] == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a851ef31e1e33d64170e3c858fc7dfe",
     "grade": false,
     "grade_id": "min",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def min_vectors(words, w2v):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "min_vectors(['great', 'wonderful'], w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc835e4bf64dd6d6b126446afa8ed211",
     "grade": true,
     "grade_id": "min2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(min_vectors(['great', 'wonderful'], w2v)[0].item(), 2) == -.03\n",
    "assert round(min_vectors(['great', 'wonderful'], w2v)[1].item(), 2) == .91\n",
    "assert min_vectors(['great', 'wonderful'], w2v).shape[0] == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f2b6849b1c3119385494cc346386470",
     "grade": false,
     "grade_id": "minmax",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def min_max_vectors(words, w2v):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "min_max_vectors(['great', 'wonderful'], w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d042928ad6d5dfcd3857568d19bcf24e",
     "grade": true,
     "grade_id": "minmax2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(min_max_vectors(['great', 'wonderful'], w2v)[0].item(), 2) == -.03\n",
    "assert round(min_max_vectors(['great', 'wonderful'], w2v)[1].item(), 2) == .91\n",
    "assert min_max_vectors(['great', 'wonderful'], w2v).shape[0] == 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's read in the movie review data from HW1 and make a bag-of-words feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.tsv', sep='\\t')\n",
    "test_df = pd.read_csv('test.tsv', sep='\\t')\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bag-of-words feature matrix for train and test, using default tokenization.\n",
    "# we remove a few terms that are not in the w2v model\n",
    "vec = CountVectorizer(max_df=100, min_df=4, binary=True,\n",
    "                      stop_words=['00am', 'hadn', 'parolini', 'virtzer', 'yelli'])\n",
    "X_train = vec.fit_transform(train_df.text)\n",
    "X_test = vec.transform(test_df.text)\n",
    "print('training matrix shape', X_train.shape, 'testing matrix shape', X_test.shape)\n",
    "y_train = train_df.label\n",
    "y_test = test_df.label\n",
    "print('training label distribution', Counter(y_train))\n",
    "print('testing label distribution', Counter(y_test))\n",
    "vocab = np.array(vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write a function that takes in a bag-of-words matrix and returns one that uses word vectors.\n",
    "Note that we'll pass in the type of aggregation function as a variable to this function. A few tips:\n",
    "\n",
    "- `vocab` is a numpy array containing the unique terms in the vocabulary\n",
    "- Recall that `X` is a [csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html). `X[i].indices` will give the indices for words contained in document i. These indices align with the order of terms in `vocab`.\n",
    "- So, you can iterate over each document, identify the terms in each document, then collect their word vectors and aggregate by using `aggregation_fn`.\n",
    "- The returned value should be a two-dimensional numpy array, where each row is a document and each column is a dimension of the aggregated word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc4c85aef4c56f24b5d3bc7a9d49820c",
     "grade": false,
     "grade_id": "featurize",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def make_word_vector_features(X, vocab, w2v, aggregation_fn):\n",
    "    \"\"\"\n",
    "    a numpy.ndarray with n rows and d columns, where n is the number of documents\n",
    "    in X and d is the word vector dimension resulting from aggregation_fn\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "            \n",
    "X_train_avg = make_word_vector_features(X_train, vocab, w2v, average_vectors)\n",
    "X_test_avg = make_word_vector_features(X_test, vocab, w2v, average_vectors)\n",
    "\n",
    "X_train_avg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af8d3f441e9f02523b6f6b162bfd1970",
     "grade": true,
     "grade_id": "cell-4d7819fc4be35413",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(float(X_train_avg[0][0]), 2) == 0.21\n",
    "assert round(float(X_train_avg[0][1]), 2) == 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll fit a LogisticRegression classifier on top of these embeddings to get our final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lr(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    fit a logistic regression classifier and report\n",
    "    accuracy on the testing data.\n",
    "    \"\"\"\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train, train_df.label)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    print(classification_report(test_df.label, y_pred_lr))\n",
    "    return lr\n",
    "\n",
    "# here is the accuracy on the original bag-of-words representation\n",
    "test_lr(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all four aggreagation functions and report acccuracy.\n",
    "for agg_fn in [average_vectors, min_vectors, max_vectors, min_max_vectors]:\n",
    "    print(agg_fn.__name__)\n",
    "    X_train_i = make_word_vector_features(X_train, vocab, w2v, agg_fn)\n",
    "    X_test_i = make_word_vector_features(X_test, vocab, w2v, agg_fn)\n",
    "    test_lr(X_train_i, y_train, X_test_i, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oof, that's a lot of work to have worse accuracy than the simple bag-of-words model! Unfortunately, it's not uncommon for this approach to not improve accuracy much for simple document classification tasks. What are possible reasons for this? What could you do to try to find evidence for one of these potential hypotheses? Enter your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2caa660884da36cc72fa2e98d4237435",
     "grade": true,
     "grade_id": "explain_bad_vectors",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intepreting word vector models\n",
    "\n",
    "In HW1, we could simply look at the top coefficients of the logistic regression model to try to understand what the model has learned. That won't work here, since a large coefficient for \"vector dimension 12\" doesn't tell us much, since we don't know what dimension 12 means.\n",
    "\n",
    "There are a number of ways to make sense of this. A simple one is the following:\n",
    "- Embed each word in the vocabulary.\n",
    "- Use the classifier to predict the class probabilities for a document containing that single word.\n",
    "- We can then use these probabilities as a way to identify terms that the classifier thinks correlate with each class.\n",
    "\n",
    "Let's start by fitting a classifier again using the average embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_avg = make_word_vector_features(X_train, vocab, w2v, average_vectors)\n",
    "X_test_avg = make_word_vector_features(X_test, vocab, w2v, average_vectors)\n",
    "lr = test_lr(X_train_avg, y_train, X_test_avg, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's embed each word in the vocabulary for later use.\n",
    "\n",
    "The function below creates a matrix where each row is a word in the vocabulary, and the columns represent its word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_words(words, w2v):\n",
    "    return np.vstack([w2v[w] for w in words])\n",
    "\n",
    "vocab_embedding = embed_words(vocab, w2v)\n",
    "print(vocab_embedding[0])\n",
    "vocab_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now classify each word in `vocab_embedding` to get the probability of the a document containing this single word of being in the positive class. You'll have to use the `predict_proba` method of [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "127a6a44345663f2706ba93594d79f9a",
     "grade": false,
     "grade_id": "posprob",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_probability_of_positive_class(vocab_embedding, lr):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "get_probability_of_positive_class(vocab_embedding, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97d31405a0a19f7846938dffa558d734",
     "grade": true,
     "grade_id": "posprob2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(get_probability_of_positive_class(vocab_embedding, lr)[0], 2) == .68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can print the top words for each class using the above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(lr, vocab_embeddings, vocab, topn=10):\n",
    "    theta = lr.predict_proba(vocab_embedding)[:,1]\n",
    "    ret = []\n",
    "    print('positive terms')\n",
    "    for i in np.argsort(theta)[::-1][:topn]:\n",
    "        print('%20s\\t%.10f' % (vocab[i], theta[i]))\n",
    "        ret.append(vocab[i])\n",
    "    print()\n",
    "    print('negative terms')\n",
    "    for i in np.argsort(theta)[:topn]:\n",
    "        print('%20s\\t%.10f' % (vocab[i], theta[i]))\n",
    "        ret.append(vocab[i])\n",
    "    return ret\n",
    "top_words = get_top_words(lr, vocab_embedding, vocab, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back at HW1 and the top terms we found with logistic regression, how do these top terms compare? Does one look better or worse? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "506b24cd796a3973983fb03ad4fbb906",
     "grade": true,
     "grade_id": "topterms",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "\n",
    "## Part II: Sequence Models\n",
    "\n",
    "Next, we'll turn to HMMs and RNNs for the task of part-of-speech tagging. \n",
    "\n",
    "Here is a simple class to store the main data structures we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, states, vocab, smoothing=0):\n",
    "        self.states = states\n",
    "        self.vocab = vocab\n",
    "        self.smoothing = smoothing\n",
    "        # we'll set these below.\n",
    "        self.transition_probas = None\n",
    "        self.emission_probas = None\n",
    "        self.start_probas = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have some training data of sentences and their corresponding part-of-speech tags:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_small = [\n",
    "                    ['the', 'boy', 'jumped'],\n",
    "                    ['the', 'dog', 'jumped'],\n",
    "                    ['jump', 'dog'],\n",
    "                    ['the', 'ball', 'boy', 'ran']\n",
    "                  ]\n",
    "\n",
    "tags_small = [\n",
    "                ['D', 'N', 'V'],\n",
    "                ['D', 'N', 'V'],\n",
    "                ['V', 'N'],\n",
    "                ['D', 'N', 'N', 'V']\n",
    "            ]\n",
    "\n",
    "states_small = ['D', 'N', 'V']\n",
    "vocab_small =['ball', 'boy', 'dog', 'jump', 'jumped', 'ran', 'the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transition probabilities\n",
    "\n",
    "First, let's implement a function to fit the transition probabilities. We'll use the add-$k$ smoothed estimates from [lecture](http://cs.tulane.edu/~aculotta/nlp/sequence/hmm2.html). To store the probabilities, we'll use a \"dict of dicts\", like:\n",
    "\n",
    "```\n",
    "{'D': {'D': 0.0, 'N': 1.0, 'V': 0.0},\n",
    " 'N': {'D': 0.0, 'N': 0.25, 'V': 0.75},\n",
    " 'V': {'D': 0.0, 'N': 1.0, 'V': 0.0}}\n",
    "```\n",
    "\n",
    "In this example, the probability of a transition from `N->V` is 0.75.\n",
    "\n",
    "Complete the function below and test your implementation. Note that transitions should appear in the result even if they are not seen in the training data. E.g., above there is a 0 probability transition from D->D. When smoothing is used, this will be non-zero.\n",
    "\n",
    "(If your object-oriented design aesthetics are offended that `fit_transition_probas` is not a method of the HMM class, then they are well-tuned. We are doing it this way solely for ease of testing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9ce671babc3184a70709310f6357476",
     "grade": false,
     "grade_id": "transition",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def fit_transition_probas(hmm, tags):\n",
    "    \"\"\"\n",
    "    Given an HMM and a dataset, return the transition probabilities between each pair of states.\n",
    "    \n",
    "    Params:\n",
    "      hmm.....an HMM\n",
    "      tags....a list of list of strings representing the POS tags for a list of sentences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48da80367b1c4b9d91a5242c005da39b",
     "grade": true,
     "grade_id": "transition1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hmm = HMM(states_small, vocab_small, smoothing=0)\n",
    "hmm.transition_probas = fit_transition_probas(hmm, tags_small)\n",
    "assert 0.750 == round(hmm.transition_probas['N']['V'], 3)\n",
    "assert 0.0 == round(hmm.transition_probas['N']['D'], 1)\n",
    "assert 0.250 == round(hmm.transition_probas['N']['N'], 3)\n",
    "\n",
    "display(dict(hmm.transition_probas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1af6ab17da51fdd049d0311c12fc8758",
     "grade": true,
     "grade_id": "transition2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hmm = HMM(states_small, vocab_small, smoothing=1)\n",
    "hmm.transition_probas = fit_transition_probas(hmm, tags_small)\n",
    "assert 0.571 == round(hmm.transition_probas['N']['V'], 3)\n",
    "assert 0.143 == round(hmm.transition_probas['N']['D'], 3)\n",
    "assert 0.286 == round(hmm.transition_probas['N']['N'], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### emission probabilities\n",
    "\n",
    "Next, let's fit the emission probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc7b319d577a4cbf5112b19194151688",
     "grade": false,
     "grade_id": "emissions",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def fit_emission_probas(hmm, sentences, tags):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea3e7a6a97cbf92c9675c00b920ef421",
     "grade": true,
     "grade_id": "emissions1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hmm = HMM(states_small, vocab_small, smoothing=0)\n",
    "hmm.emission_probas = fit_emission_probas(hmm, sentences_small, tags_small)\n",
    "assert 0.2 == round(hmm.emission_probas['N']['ball'], 1)\n",
    "assert 0.4 == round(hmm.emission_probas['N']['boy'], 1)\n",
    "assert 0.4 == round(hmm.emission_probas['N']['dog'], 1)\n",
    "assert 0.0 == round(hmm.emission_probas['N']['jump'], 1)\n",
    "assert 0.0 == round(hmm.emission_probas['N']['ran'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6361a7e685626d59d119c6f1fca8ff4f",
     "grade": true,
     "grade_id": "emissions2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hmm = HMM(states_small, vocab_small, smoothing=1)\n",
    "hmm.emission_probas = fit_emission_probas(hmm, sentences_small, tags_small)\n",
    "assert 0.4 == round(hmm.emission_probas['D']['the'], 1)\n",
    "assert 0.1 == round(hmm.emission_probas['D']['ball'], 1)\n",
    "assert 0.1 == round(hmm.emission_probas['D']['boy'], 1)\n",
    "assert 0.1 == round(hmm.emission_probas['D']['jump'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e58489ba0645775d3a39e4c85910bc85",
     "grade": true,
     "grade_id": "emissions3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start probabilities\n",
    "\n",
    "The final probabilities we need to estimate is the start probabilities. That is, what is the probability of a tag starting a sentence? (For this assignment, we'll ignore end states.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bfaa80c9704e0448719c3036f95d2d0",
     "grade": false,
     "grade_id": "start_probas",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def fit_start_probas(hmm, tags):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88460dbc605201a577e07db235a8052e",
     "grade": true,
     "grade_id": "start_probas2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hmm = HMM(states_small, vocab_small, smoothing=0)\n",
    "hmm.start_probas = fit_start_probas(hmm, tags_small)\n",
    "assert 0.75 == round(hmm.start_probas['D'], 2)\n",
    "assert 0.0 == round(hmm.start_probas['N'], 1)\n",
    "assert 0.25 == round(hmm.start_probas['V'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi\n",
    "\n",
    "Now that we can estimate all the HMM parameters, we'll implement Viterbi search.\n",
    "\n",
    "This function will return both the most probable state sequence as well as its probability.\n",
    "\n",
    "See the book for more details. One issue is keeping track of which state was chosen at each step. You can do this with a \"backpointer\" matrix to keep track of the best edges. This is a 2d array just like the lattice used by Viterbi. Cell (i,j) stores for state i at time step j the best preceding state at time j-1. That is, it indicates which edge was taken to reach state i at time j. (See [Figure A.10](https://github.com/tulane-cmps6730/main/blob/main/read/slpA.pdf) from the book.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4219571724d7ea459181c9b7b624172f",
     "grade": false,
     "grade_id": "viterbi",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(hmm, sentence):\n",
    "    \"\"\"\n",
    "    Perform Viterbi search to identify the most probable set of hidden states for\n",
    "    the provided input sentence.\n",
    "    \n",
    "    Be sure that the runtime of your implementation is O(N^2 * T), where N is\n",
    "    the number of states and T is the number of sentence tokens.\n",
    "    \n",
    "    \n",
    "\n",
    "    Params:\n",
    "      sentence...a lists of strings, representing the tokens in a single sentence.\n",
    "\n",
    "    Returns:\n",
    "      path....a list of strings indicating the most probable path of POS tags for\n",
    "              this sentence.\n",
    "      proba...a float indicating the probability of this path.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0e388a86a8f49b6bd994d2f5d833175",
     "grade": true,
     "grade_id": "viterbi1",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To test our implementation, we'll consider the sentence \"time flies like an arrow\".\n",
    "There is ambiguity in the word \"flies\", which could be a noun or verb, as well\n",
    "as the word \"like,\" which could be a preposition or a verb. \n",
    "\"\"\"\n",
    "hmm = HMM(states=['D', 'N', 'P', 'V'],\n",
    "          vocab=['an', 'arrow', 'flies', 'like', 'time'],\n",
    "          smoothing=0)\n",
    "\n",
    "hmm.start_probas = {\n",
    "                    'D': .3,\n",
    "                    'N': .4,\n",
    "                    'P': .1,\n",
    "                    'V': .2,\n",
    "                   }\n",
    "\n",
    "hmm.emission_probas = {\n",
    "                         'D': {'time': 0.0, 'flies': 0.0, 'like': 0.0, 'an': 1.0, 'arrow': 0.0},\n",
    "                         'V': {'time': 0.0, 'flies': 0.5, 'like': 0.5, 'an': 0.0, 'arrow': 0.0},\n",
    "                         'P': {'time': 0.0, 'flies': 0.0, 'like': 1.0, 'an': 0.0, 'arrow': 0.0},\n",
    "                         'N': {'time': 0.3, 'flies': 0.3, 'like': 0.0, 'an': 0.0, 'arrow': 0.4}\n",
    "                      }\n",
    "hmm.transition_probas = {  \n",
    "                           'D': {'D': 0.0, 'N': 1.0, 'P': 0.0, 'V': 0.0},\n",
    "                           'N': {'D': 0.0, 'N': 0.3, 'P': 0.2, 'V': 0.5},\n",
    "                           'P': {'D': 0.8, 'N': 0.2, 'P': 0.0, 'V': 0.0},\n",
    "                           'V': {'D': 0.2, 'N': 0.5, 'P': 0.3, 'V': 0.0}\n",
    "                        }\n",
    "\n",
    "path, proba = viterbi(hmm, ['time', 'flies', 'like', 'an', 'arrow'])\n",
    "assert list(path) ==  ['N', 'V', 'P', 'D', 'N']\n",
    "assert .003 == round(proba, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "059a99ae99b915934121d7a19bf3c9e5",
     "grade": true,
     "grade_id": "viterbi2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e4ef0b8fb0d52c255ba2d37f2366ffd",
     "grade": false,
     "grade_id": "second-best-d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sometimes we want to find the second most probable output path for an input sentence. In the cell below, describe in words an algorithm to find the second most probable path. What is its Big-Oh runtime?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fec681361e1655f08106d9b581af8b48",
     "grade": true,
     "grade_id": "second-best",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running your HMM on real data\n",
    "\n",
    "Now that your HMM is working, we'll apply it to some real data. The file `pos.txt` contains sentences from news articles that have been annotated with their parts-of-speech. A list of part-of-speech tag definitions can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labeled_data(filename):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    cur_sentence = []\n",
    "    cur_tags = []\n",
    "    for line in open(filename):\n",
    "        parts = line.split()\n",
    "        if len(parts) < 2: # next sentence\n",
    "            sentences.append(cur_sentence)\n",
    "            tags.append(cur_tags)\n",
    "            cur_sentence = []\n",
    "            cur_tags = []\n",
    "        else:\n",
    "            cur_sentence.append(parts[0])\n",
    "            cur_tags.append(parts[1])\n",
    "    return sentences, tags\n",
    "sentences, tags = read_labeled_data('pos.txt')\n",
    "print('read %d sentences containing %d tokens' % (len(sentences), sum(len(s) for s in sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unique set of states\n",
    "states = set()\n",
    "for tag_list in tags:\n",
    "    states.update(tag_list)\n",
    "states = sorted(list(states))\n",
    "print('%d unique states' % len(states))\n",
    "\n",
    "# find the vocabulary\n",
    "vocab = set()\n",
    "for sentence in sentences:\n",
    "    vocab.update(sentence)\n",
    "vocab = sorted(list(vocab))\n",
    "print('%d unique words' % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4bfcac9973eb3c16cd73a5a85f372f6",
     "grade": true,
     "grade_id": "hmm-all",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_sentences, test_sentences = sentences[:-10], sentences[-10:]\n",
    "train_tags, test_tags = tags[:-10], tags[-10:]\n",
    "\n",
    "hmm = HMM(states=states, vocab=vocab, smoothing=0.001)\n",
    "hmm.start_probas = fit_start_probas(hmm, train_tags)\n",
    "hmm.transition_probas = fit_transition_probas(hmm, train_tags)\n",
    "hmm.emission_probas = fit_emission_probas(hmm, train_sentences, train_tags)\n",
    "sentence = ['Look', 'at', 'what', 'happened']\n",
    "print('predicted parts of speech for the sentence %s' % str(sentence))\n",
    "path, proba = viterbi(hmm, sentence)\n",
    "print(path, proba)\n",
    "assert path == ['VB', 'IN', 'WP', 'VBD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to evaluate accuracy of an HMM. A simple way is to compute the accuracy averaged over all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(hmm, sentences, tags):\n",
    "    truths = []\n",
    "    preds = []\n",
    "    for sentence, tag_list in zip(sentences, tags):\n",
    "        path, proba = viterbi(hmm, sentence)\n",
    "        truths.extend(tag_list)\n",
    "        preds.extend(path)\n",
    "    print(classification_report(truths, preds, zero_division=0))\n",
    "    \n",
    "evaluate(hmm, test_sentences, test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will happen if the HMM encounters a word that is not in `vocab`?  How could you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3abb38429edf5161d5a3d977364bc83",
     "grade": true,
     "grade_id": "missing-word",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: RNNs for POS tagging\n",
    "\n",
    "Finally, we'll apply RNNs to this task. Below is a slightly modified version of the RNN we introduced in lecture.\n",
    "\n",
    "We'll first train it using the `small` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, states, verbose=True):\n",
    "        super(RNN, self).__init__()\n",
    "        self.states = states\n",
    "        self.verbose = verbose\n",
    "        self.n_hidden = hidden_size\n",
    "        # W_xh\n",
    "        self.input_to_hidden = nn.Linear(input_size, hidden_size)\n",
    "        # W_hh\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        # W_hy\n",
    "        self.hidden_to_output = nn.Linear(hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        # tanh(ð‘Š_hh h_t-1 + W_xh x_t)\n",
    "        h_new = self.tanh(self.hidden_to_hidden(h_prev) + self.input_to_hidden(x))\n",
    "        # y_h = softmax(W_hy h)\n",
    "        output = self.softmax(self.hidden_to_output(h_new))\n",
    "        \n",
    "        if self.verbose: print('h_new\\n', h_new, '\\noutput\\n',output, '\\ntop output\\n', output2label(output, self.states))\n",
    "        return output, h_new\n",
    "    \n",
    "    def forward_unrolled(self, x_list):\n",
    "        \"\"\"\n",
    "        Given a character list, unroll the network and gather\n",
    "        predictions for each time step.\n",
    "        \"\"\"\n",
    "        hidden = torch.zeros(1, self.n_hidden)\n",
    "        outputs = []\n",
    "        hiddens = []\n",
    "        for x in x_list:\n",
    "            output, hidden = self.forward(x, hidden)\n",
    "            outputs.append(output)\n",
    "            hiddens.append(hidden)\n",
    "        return torch.cat(outputs), hidden\n",
    "    \n",
    "def output2label(output, states):\n",
    "    \"\"\"\n",
    "    return the letter for the most probable prediction\n",
    "    \"\"\"\n",
    "    top_n, top_i = output.topk(1)\n",
    "    return states[top_i.item()] # , math.exp(top_n)\n",
    "\n",
    "\n",
    "def one_hot(vocab, word):\n",
    "    x = torch.zeros((1, len(vocab)))\n",
    "    x[0][vocab.index(word)] = 1.0\n",
    "    return x\n",
    "\n",
    "def one_hots(vocab, words):\n",
    "    return torch.cat([one_hot(vocab, w) for w in words])\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "rnn = RNN(len(vocab_small), 2, len(states_small), states_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a prediction for one sentence, with random weights\n",
    "outputs, hiddens = rnn.forward_unrolled(one_hots(vocab_small, ['the', 'boy', 'jumped']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll have to one-hot-encode both the input words and output states to create the data.\n",
    "rnn_data_small = [(one_hots(vocab_small, sentence), one_hots(states_small, tag_list)) \n",
    "                    for sentence, tag_list in zip(sentences_small, tags_small)]\n",
    "rnn_data_small[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, train_data, test_data, epochs=20, learning_rate=0.1):\n",
    "    torch.random.manual_seed(42)  # for reproducibility\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(),\n",
    "                                 lr=learning_rate) \n",
    "    rnn.verbose = False\n",
    "    loss_val = []\n",
    "    test_acc = []\n",
    "    # main training loop\n",
    "    # here, we're updating after each sentence.\n",
    "    # to improve convergence, we should really use mini-batches (e.g., a group of ~20 sentences)\n",
    "    # but, this is a bit easier.\n",
    "    for epoch in tqdm(range(epochs), total=epochs):\n",
    "        for sentence, tags in train_data:\n",
    "            optimizer.zero_grad()                             # reset all the gradient information\n",
    "            outputs, hiddens = rnn.forward_unrolled(sentence) # predict on this sentence\n",
    "            loss = criterion(outputs, tags)                   # compute loss\n",
    "            loss.backward()                                   # computes all the gradients\n",
    "            optimizer.step()                                  # update parameters\n",
    "            loss_val.append(loss.item())                      # track loss on each sentence\n",
    "        # after each epoch, record the test accuracy.\n",
    "        test_acc.append(evaluate_rnn(rnn, test_data, verbose=False))\n",
    "\n",
    "    # plot training loss\n",
    "    plt.figure()\n",
    "    plt.plot(loss_val, 'bo-')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('instance')\n",
    "    plt.title('training')\n",
    "    plt.show()\n",
    "    # plot testing F1\n",
    "    plt.figure()\n",
    "    plt.plot(test_acc, 'bo-')\n",
    "    plt.ylabel('f1')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('testing')\n",
    "    plt.show()\n",
    "    \n",
    "    return rnn\n",
    "\n",
    "\n",
    "def evaluate_rnn(rnn, data, verbose=True):\n",
    "    rnn.verbose = False\n",
    "    truths = []\n",
    "    preds = []\n",
    "    for sentence, tag_list in data:\n",
    "        outputs, _ = rnn.forward_unrolled(sentence)\n",
    "        pred = [output2label(output, rnn.states) for output in outputs]\n",
    "        truth = [output2label(tag, rnn.states) for tag in tag_list]\n",
    "        truths.extend(truth)\n",
    "        preds.extend(pred)\n",
    "    if verbose:\n",
    "        print(classification_report(truths, preds, zero_division=0))\n",
    "    return f1_score(truths, preds, average='weighted')\n",
    "\n",
    "rnn = RNN(len(vocab_small), 2, len(states_small), states_small)\n",
    "train_rnn(rnn, rnn_data_small, rnn_data_small, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN seems to have no problem fitting this tiny dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's one prediction of the trained RNN\n",
    "rnn.verbose=True\n",
    "_, _ = rnn.forward_unrolled(one_hots(vocab_small, ['the', 'boy', 'jumped']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs on real data.\n",
    "\n",
    "Next, we'll try the larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encode the larger POS dataset\n",
    "rnn_data = [(one_hots(vocab, sentence), one_hots(states, tag_list)) \n",
    "            for sentence, tag_list in zip(sentences, tags)]\n",
    "rnn_data_train, rnn_data_test = rnn_data[:-10], rnn_data[-10:]\n",
    "print('%d training and %d testing sentences' % (len(rnn_data_train), len(rnn_data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(len(vocab), 5, len(states), states)\n",
    "train_rnn(rnn, rnn_data_train, rnn_data_test, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on one example.\n",
    "rnn.verbose=True\n",
    "_, _ = rnn.forward_unrolled(one_hots(vocab, ['Look', 'at', 'what', 'happened']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_rnn(rnn, rnn_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs with word embeddings\n",
    "\n",
    "The RNN seems pretty comparable with the HMM. As usual, more data and more tuning would likely lead to higher accuracy for the RNN.\n",
    "\n",
    "Next, we can use the word embeddings as the input representation for each word, rather than the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mean vector if word not in w2v\n",
    "MEAN_VEC = w2v.vectors.mean(axis=0)\n",
    "\n",
    "def embed_words_rnn(w2v, sentence):\n",
    "    \"\"\"\n",
    "    Embed each token with its vector from w2v.\n",
    "    \"\"\"\n",
    "    return torch.tensor([w2v[w.lower()] if w.lower() in w2v else MEAN_VEC for w in sentence])\n",
    "    \n",
    "rnn_data_embed = [(embed_words_rnn(w2v, sentence), one_hots(states, tag_list)) \n",
    "                    for sentence, tag_list in zip(sentences, tags)]\n",
    "rnn_data_embed_train, rnn_data_embed_test = rnn_data_embed[:-10], rnn_data_embed[-10:]\n",
    "print('%d training and %d testing sentences' % (len(rnn_data_embed_train), len(rnn_data_embed_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input dimension has now changed to 50, the size of the word embedding\n",
    "rnn = RNN(50, 10, len(states), states)\n",
    "train_rnn(rnn, rnn_data_embed_train, rnn_data_embed_test, epochs=15, learning_rate=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_rnn(rnn, rnn_data_embed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...at first glance it doesn't seem worth using an RNN with this data. But, maybe we haven't tuned it properly?\n",
    "\n",
    "Play around with the learning rate, hidden_size, and number of epochs for the final RNN model above. What is the best \"weighted avg\" F1 you can obtain? What are the values of the tuning parameters you use? One tip -- as you increase hidden size, you may need to reduce the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42aeccc9eaf711f49b7f4baccfce39e6",
     "grade": true,
     "grade_id": "rnn-tune",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
